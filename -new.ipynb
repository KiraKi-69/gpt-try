{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48fb7e5-6ee8-40a1-950f-771b4c8f55d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71cd46c-32ce-42e9-9b59-4a0d07c71791",
   "metadata": {},
   "source": [
    "### Подключение к mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd34c974-ee20-4cdb-96e5-e58ce9ac723d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a44d12-3c09-4e9b-a8cb-109326b49e7b",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7274965-cc5e-4ae7-bdf6-3db5cbc85fd9",
   "metadata": {},
   "source": [
    "В нашем демо мы будем использовать ruGPT3 модели от Сбербанка. Работать с моделями можно выгружая их из хаба HuggingFace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4722294-bf32-4b3e-a041-d054a890c46a",
   "metadata": {},
   "source": [
    "Мы можем загрузить модель из несколькими способами.\n",
    "* Напрямую выгружая из дистрибутива HuggingFace с помощью библиотеки transformers\n",
    "* Использовать для выгрузки langchain и его функцию HuggingFacePipeline\n",
    "* Использовать langchain и его функцию HuggingFaceHub (требуется API токен)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5bce55-0bf4-404c-a955-8abeb33681b8",
   "metadata": {},
   "source": [
    "Так же мы будем использовать chains из библиотеки langchain для связи пар модель\\промпт."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a04ca8-b0a6-4246-9405-f1a15b3c9d0c",
   "metadata": {},
   "source": [
    "## 1. Выгрузка модели с помощью transformers и инициализация chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2d5dbe-d96b-4a79-91ee-38b94d51ad55",
   "metadata": {},
   "source": [
    "### Выгружаем модель через transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5708e929-1dc8-41ae-8bac-2f15fde0d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf814c26-0349-4838-b32d-422ec050ec27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3dc7048-8785-4cfc-b9af-c05fac8d0d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc49406bdf0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8546f882-becd-4cc5-93bd-b00b9900e977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "801a33bf-4f8a-4803-990d-eb7bbf1f9180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer_and_model(model_name_or_path):\n",
    "    return GPT2Tokenizer.from_pretrained(model_name_or_path), GPT2LMHeadModel.from_pretrained(model_name_or_path).cuda()\n",
    "\n",
    "\n",
    "def generate(\n",
    "    model, tok, text,\n",
    "    do_sample=True, max_length=100, repetition_penalty=5.0,\n",
    "    top_k=5, top_p=0.95, temperature=1,\n",
    "    num_beams=None,\n",
    "    no_repeat_ngram_size=3\n",
    "    ):\n",
    "    input_ids = tok.encode(text, return_tensors=\"pt\").cuda()\n",
    "    out = model.generate(\n",
    "      input_ids.cuda(),\n",
    "      max_length=max_length,\n",
    "      repetition_penalty=repetition_penalty,\n",
    "      do_sample=do_sample,\n",
    "      top_k=top_k, top_p=top_p, temperature=temperature,\n",
    "      num_beams=num_beams, no_repeat_ngram_size=no_repeat_ngram_size\n",
    "      )\n",
    "    return list(map(tok.decode, out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2383d3c-7572-442b-b1dd-2705657a9117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tok, model = load_tokenizer_and_model(\"sberbank-ai/rugpt3small_based_on_gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94976471-b239-414f-a54b-8513f3cf875e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "generated = generate(model, tok, \"Как приготовить борщ?\", num_beams=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f123b823-fa83-4b76-8651-b0e52bcf542a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Как приготовить борщ?\\nБорщ - это блюдо, которое готовится на пару. Борщ можно подать как самостоятельное блюдо, так и в качестве гарнира к мясным или рыбным блюдам. Для приготовления борща понадобятся следующие продукты: 1 кг мяса (лучше говядины) 2 луковицы 3 ст. л. растительного масла 4 зубчика чеснока соль по вкусу лавровый лист черный молотый перец Способ приготовления: Мясо пропустить через мясорубку вместе с луком до образования однородной'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3da94f-86a0-4881-8710-7f9f0dab8a9b",
   "metadata": {},
   "source": [
    "### LangChain wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6516fa4c-04c6-486b-82f5-1a11fd2559db",
   "metadata": {},
   "source": [
    "Чтобы создать chain нам потребуется обернуть нашу выгруженную модель класс langchain модели, для этого используем wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4414b61c-cbd2-424b-843c-4978a1108f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "135bf525-02e0-4d42-8222-1f328b2d9657",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLLM(LLM):\n",
    "\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "    \n",
    "    model_name_or_path = \"sberbank-ai/rugpt3small_based_on_gpt2\"\n",
    "    tok, model = GPT2Tokenizer.from_pretrained(model_name_or_path), GPT2LMHeadModel.from_pretrained(model_name_or_path).cuda()\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"self_hosted_hugging_face\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "    ) -> str:\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "\n",
    "\n",
    "        input_ids = self.tok.encode(prompt, return_tensors=\"pt\").cuda()\n",
    "\n",
    "        out = self.model.generate(\n",
    "            input_ids.cuda(),\n",
    "            max_length=200,\n",
    "            repetition_penalty=5.0,\n",
    "            do_sample=True,\n",
    "            top_k=5, \n",
    "            top_p=0.95, \n",
    "            temperature=1,\n",
    "            num_beams=10, \n",
    "            no_repeat_ngram_size=3\n",
    "      )\n",
    "        \n",
    "        return list(map(tok.decode, out))[0]\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\"model name\": self.model_name_or_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56ab46b4-df49-467a-afbd-7690a0484c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruLLM = CustomLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21d33571-d786-4d75-94fe-bb6048616088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Как приготовить борщ?\\nБорщ - это блюдо, которое готовится в мультиварке. Борщ можно подать как самостоятельное блюдо, так и на гарнир к мясу или рыбе. Для приготовления борща понадобятся следующие продукты: 1 кг свеклы; 2 ст. л. томатного соуса; 3-4 зубчика чеснока; зелень петрушки; соль по вкусу; растительное масло для жарки; уксусная эссенция (1 ч. л.). Все ингредиенты тщательно перемешиваем до получения однородной консистенции. На сковороде разогреваем оливковое масло, обжариваем его с двух сторон до образования золотистой корочки. Затем добавляем нарезанный кубиками репчатый лук, чеснок, лавровый лист, перец горошком, солим и перчим по вкусу. В сковороду наливаем подсолнечное масло, накрываем крышкой и тушим около 10 минут. Готовый борщ готов! Приятного аппетита!Рецепт http'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ruLLM(\"Как приготовить борщ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07e58c83-ba0a-4e24-94b6-c2f77ffb4866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCustomLLM\u001b[0m\n",
      "Params: {'model name': 'sberbank-ai/rugpt3small_based_on_gpt2'}\n"
     ]
    }
   ],
   "source": [
    "print(ruLLM)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f901cdc-97ad-4e12-9714-9e737ff49d4f",
   "metadata": {},
   "source": [
    "### LangChain chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cde5d57-1d36-4d72-a701-edc42c47725f",
   "metadata": {},
   "source": [
    "Создаем связь model\\prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b870f65-d9c3-4a44-a927-2dff23f386d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c94422ba-bc0d-4705-ab0b-c57857072629",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"Как приготовить {product}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f982af6-11b8-47ac-9d6b-1aa60b93371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=ruLLM, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae5ee03a-2fa6-4f15-b07d-35945750ff15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Как приготовить борщ?\n",
      "Борщ - это блюдо, которое готовится в мультиварке. Борщ можно подать как самостоятельное блюдо, так и на гарнир к мясу или рыбе. Для приготовления борща понадобятся следующие продукты: 1 кг свеклы; 2 ст. л. растительного масла; 3-4 зубчика чеснока; 0,5 стакана томатной пасты; 100 мл воды; соль по вкусу. В кастрюлю с толстым дном наливаем растительное масло, доводим до кипения и варим около 10 минут. Затем добавляем нарезанный кубиками репчатый лук, лавровый лист, черный молотый перец горошком (по желанию) и все тщательно перемешиваем. Суп готов! Приятного аппетита!\n",
      "Свеклу очистить от кожуры и нарезать тонкими ломтиками. Морковь нашинковать соломкой. Чеснок мелко порубить. На сковороде растопить сливочное масло и обжарить свеклу со всех сторон. Когда свекла станет мягкой\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"борщ\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2265e354-cfa1-4461-bb9f-e3a7df8e86c5",
   "metadata": {},
   "source": [
    "## 2. Выгрузка модели с помощью langchain и HuggingFacePipeline и инициализация chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba137b3-919e-4488-8579-9ee36ad366fa",
   "metadata": {},
   "source": [
    "LangChain предоставляет удобное API для выгрузки моделей из HuggingFaceHub. Одна из таких функций - HuggingFacePipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fdab53-1b86-4382-bf41-8a1c8252f8fb",
   "metadata": {},
   "source": [
    "### Выгружаем модель с помощью HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39c396dd-6c9d-4cba-afe7-10faa5242b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebb8c00d-1fa1-4a50-97dd-e3ccacca7e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device has 1 GPUs available. Provide device={deviceId} to `from_model_id` to use availableGPUs for execution. deviceId is -1 (default) for CPU and can be a positive integer associated with CUDA device id.\n"
     ]
    }
   ],
   "source": [
    "hfp = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={ \n",
    "        \"max_length\": 100,\n",
    "        \"repetition_penalty\": 5.0,\n",
    "        \"do_sample\": True,\n",
    "        \"top_k\": 5, \n",
    "        \"top_p\": 0.95, \n",
    "        \"temperature\": 1,\n",
    "        \"num_beams\": 10, \n",
    "        \"no_repeat_ngram_size\": 3},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d74c1bc-b51c-4455-8e3d-e21a571b6afe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Pipeline with tokenizer without pad_token cannot do batching. You can try to set it with `pipe.tokenizer.pad_token_id = model.config.eos_token_id`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mhfp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mКак приготовить борщ?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/llms/base.py:870\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    864\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`generate` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    868\u001b[0m     )\n\u001b[1;32m    869\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 870\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    880\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/llms/base.py:650\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    636\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    637\u001b[0m         )\n\u001b[1;32m    638\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    639\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    640\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    648\u001b[0m         )\n\u001b[1;32m    649\u001b[0m     ]\n\u001b[0;32m--> 650\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/llms/base.py:538\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    537\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 538\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    539\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/llms/base.py:525\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    517\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    522\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    524\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 525\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    529\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    532\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    533\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    534\u001b[0m         )\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain/llms/huggingface_pipeline.py:195\u001b[0m, in \u001b[0;36mHuggingFacePipeline._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m batch_prompts \u001b[38;5;241m=\u001b[39m prompts[i : i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size]\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# Process batch of prompts\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_prompts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# Process each response in the batch\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(responses):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:208\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    168\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1118\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list:\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[0;32m-> 1118\u001b[0m         final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(final_iterator)\n\u001b[1;32m   1122\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1071\u001b[0m, in \u001b[0;36mPipeline.get_iterator\u001b[0;34m(self, inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;66;03m# TODO hack by collating feature_extractor and image_processor\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m feature_extractor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_processor\n\u001b[0;32m-> 1071\u001b[0m collate_fn \u001b[38;5;241m=\u001b[39m no_collate_fn \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mpad_collate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1072\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, num_workers\u001b[38;5;241m=\u001b[39mnum_workers, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn)\n\u001b[1;32m   1073\u001b[0m model_iterator \u001b[38;5;241m=\u001b[39m PipelineIterator(dataloader, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward, forward_params, loader_batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:132\u001b[0m, in \u001b[0;36mpad_collate_fn\u001b[0;34m(tokenizer, feature_extractor)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    133\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline with tokenizer without pad_token cannot do batching. You can try to set it with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pipe.tokenizer.pad_token_id = model.config.eos_token_id`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    135\u001b[0m         )\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m         t_padding_value \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id\n",
      "\u001b[0;31mValueError\u001b[0m: Pipeline with tokenizer without pad_token cannot do batching. You can try to set it with `pipe.tokenizer.pad_token_id = model.config.eos_token_id`."
     ]
    }
   ],
   "source": [
    "hfp(\"Как приготовить борщ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e31841-befd-487f-b47e-3c667f3b0ea4",
   "metadata": {},
   "source": [
    "### HuggingFacePipeline chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa84c2c0-6fe5-4c56-9fed-21f886614cc3",
   "metadata": {},
   "source": [
    "Создаем связь model\\prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d83bc20-870c-4d43-a272-58a499a26340",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"Как приготовить {product}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e2c201-148c-4d60-bc9d-a3f546bbb6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hfp_chain = LLMChain(llm=hfp, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db42381-7e10-4d00-8040-a197b9704793",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hfp_chain.run(\"борщ\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa90c4b0-3c7e-4ef7-9ae9-ac1819f2effb",
   "metadata": {},
   "source": [
    "## 3. Выгрузка модели с помощью langchain и HuggingFaceHub и инициализация chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4f6cbe-e0a8-4ceb-84a8-a536d0f65050",
   "metadata": {},
   "source": [
    "Так же модели можно выгружать через функцию HuggingFaceHub, но для этого потребуется api token аккаунта"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50539e3c-23ae-4c12-97fe-6956678dbc3c",
   "metadata": {},
   "source": [
    "### Выгружаем модель с помощью HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a8767fb-271e-4218-8ba5-5bdd40c5a4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cbb6736b-4c80-4ce0-9270-d79758564594",
   "metadata": {},
   "outputs": [],
   "source": [
    "hfh = HuggingFaceHub(\n",
    "    repo_id=\"sberbank-ai/rugpt3small_based_on_gpt2\", \n",
    "    huggingfacehub_api_token=\"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\",\n",
    "    model_kwargs={ \n",
    "        \"max_length\": 100,\n",
    "        \"repetition_penalty\": 5.0,\n",
    "        \"do_sample\": True,\n",
    "        \"top_k\": 5, \n",
    "        \"top_p\": 0.95, \n",
    "        \"temperature\": 1,\n",
    "        \"num_beams\": 10, \n",
    "        \"no_repeat_ngram_size\": 3},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1bee9366-0bbb-4c7e-b314-84b8820205a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nБорщ - это блюдо из мяса, рыбы и овощей. Борщ можно подать как самостоятельное блюдо, так и в качестве гарнира к мясным или овощным блюдам. Для приготовления борща понадобятся следующие продукты: говядина, свинина, баранина, курица, картофель, лук, морковь, свекла, томатная паста, лавровый лист, перец горошком, соль, черный молотый перец (по вкусу), растительное масло для жар'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hfh(\"Как приготовить борщ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14f0f2d-1018-44ea-b1ba-1a000a897661",
   "metadata": {},
   "source": [
    "### HuggingFaceHub chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053efd1a-bfb8-4386-a9d4-fa3a55a2b7e6",
   "metadata": {},
   "source": [
    "Создаем связь model\\prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb1171e2-bb96-4978-9d81-e95da1019d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"Как приготовить {product}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ca3c8cb-05e4-4be2-b03c-ccbebda68a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hfh_chain = LLMChain(llm=hfh, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "daaa4da6-aeed-4e10-8ba1-be890989c7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Борщ - это блюдо из мяса, рыбы и овощей. Борщ можно подать как самостоятельное блюдо, так и в качестве гарнира к мясным или овощным блюдам. Для приготовления борща понадобятся следующие продукты: говядина, свинина, баранина, курица, картофель, лук, морковь, свекла, томатная паста, лавровый лист, перец горошком, соль, черный молотый перец (по вкусу), растительное масло для жар\n"
     ]
    }
   ],
   "source": [
    "print(hfh_chain.run(\"борщ\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf950e8d-014b-4b91-ae7d-3802053c64a5",
   "metadata": {},
   "source": [
    "# MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e63984c-723d-4863-9865-e07df3571319",
   "metadata": {},
   "source": [
    "MLFlow предоставляет api для работы с LLM моделями. Рассмотрим некоторые его функции:\n",
    "* Сохранение и выгрузка chains для повторного многократного использования\n",
    "* Логирование входов и выходов модели для валидации\n",
    "* Сравнение выводов нескольких моделей с помощью функции evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8432c87c-2099-4d95-ab4c-74d9e8bfebf8",
   "metadata": {},
   "source": [
    "## Сохранение и выгрузка chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c392535-c5d2-4cbb-bbf8-43b38186f860",
   "metadata": {},
   "source": [
    "Сохраненные chains можно сохранять для повторного использования. В качестве шаблона можно передавать модели указания как вести себя с пользователем или инструкции к переводу входной строки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "510cc75a-8e64-432d-b01f-7a8bf800ab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow \n",
    "import os\n",
    "import langchain.agents\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2957086-45b7-4740-8c5d-787bff7ff533",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='s3://jojo-gpt/1', creation_time=1698738735342, experiment_id='1', last_update_time=1698738735342, lifecycle_stage='active', name='LLM_chain', tags={}>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"LLM_chain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c84a825-7d5a-45cb-aa72-ffc67e2e2aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"log_chain\") as run:\n",
    "    mlflow.langchain.log_model(hfh_chain, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0780cd8f-d05b-4f86-9bd0-bdc58b83d448",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e88985650c3c497fb90456a2497c28c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_hfh = mlflow.langchain.load_model(\"s3://jojo-gpt/1/e0a4c9b9553e4abe9b3cf0870c3d93ee/artifacts/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "893ba664-2116-44e1-99b4-43abe4870f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nСварить бульон, добавить в него мелко нарезанный репчатый лук и обжарить на сковороде с растительным маслом до золотистого цвета. В конце варки добавить лавровый лист и специи по вкусу. На гарнир можно подать картофельное пюре или отварной рис. Приятного аппетита!\\nПРИЯТНОГО АППЕТИТА!!! Суп-пюре из щавеля готовится очень просто: 1 луковица 2 ст. л.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_hfh.run(\"щи\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cbe2c3-f9c0-4b4a-bf08-1357dec1f354",
   "metadata": {},
   "source": [
    "## Логирование входов и выходов модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbf2ef5-26ef-4fb3-ba1e-af6ae416f325",
   "metadata": {},
   "source": [
    "Используется для логирования ответов для валидации модели. Здесь мы будем использовать не chain, а саму модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8321d394-6cbd-4c31-bb89-2e6df1b86f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "48225c19-0143-4d18-8f97-5d12711b3234",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/10/31 07:57:40 INFO mlflow.tracking.fluent: Experiment with name 'Log_predictions' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='s3://jojo-gpt/2', creation_time=1698739060123, experiment_id='2', last_update_time=1698739060123, lifecycle_stage='active', name='Log_predictions', tags={}>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"Log_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9a11c823-967d-49a4-9750-de98d811e881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "inputs = [ \n",
    "    \"борщ\",\n",
    "]\n",
    "outputs = [\n",
    "    ruLLM(\"Как приготовить борщ?\"),\n",
    "]\n",
    "prompts = [\n",
    "    \"Как приготовить {input}?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "11899bc5-691b-4edb-99b4-6780eb1af8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/10/31 07:57:48 INFO mlflow.tracking.llm_utils: Creating a new llm_predictions.csv for run 7ca769b529f94066b89a8cf98b26752e.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"log_predictions\"):\n",
    "    mlflow.llm.log_predictions(inputs, outputs, prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bea2d0-0b85-4ab1-b20b-18b3d23cda0a",
   "metadata": {},
   "source": [
    "## Сравнение выводов нескольких моделей с помощью функции evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3da91a-65a7-4052-a6b4-9fa9b1ed8e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyfuncTransformer(mlflow.pyfunc.PythonModel):\n",
    "\n",
    "    def __init__(self, model_name):\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import torch\n",
    "        from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "        self.model_name = model_name\n",
    "        super().__init__()\n",
    "\n",
    "    def load_context(self, context):\n",
    "\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(self.model_name)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(self.model_name).cuda()\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        import pandas as pd\n",
    "        if isinstance(model_input, pd.DataFrame):\n",
    "            model_input = model_input.values.flatten().tolist()\n",
    "        elif not isinstance(model_input, list):\n",
    "            model_input = [model_input]\n",
    "\n",
    "        generated_text = []\n",
    "        for input_text in model_input:\n",
    "\n",
    "            input_ids = self.tokenizer.encode(input_text, return_tensors=\"pt\").cuda()\n",
    "\n",
    "            out = self.model.generate(\n",
    "                input_ids.cuda(),\n",
    "                max_length=100,\n",
    "                repetition_penalty=5.0,\n",
    "                do_sample=True,\n",
    "                top_k=5, \n",
    "                top_p=0.95, \n",
    "                temperature=1,\n",
    "                num_beams=10, \n",
    "                no_repeat_ngram_size=3\n",
    "              )\n",
    "\n",
    "            generated_text.append(\n",
    "                list(map(self.tokenizer.decode, out))[0],\n",
    "            )\n",
    "\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56ca030-35e1-4b18-aabd-ca5b38a4235b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rugpt3small = PyfuncTransformer(\n",
    "    \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
    ")\n",
    "rugpt3medium = PyfuncTransformer(\n",
    "    \"sberbank-ai/rugpt3medium_based_on_gpt2\",\n",
    ")\n",
    "rugpt3large = PyfuncTransformer(\n",
    "    \"sberbank-ai/rugpt3large_based_on_gpt2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b1cd31-e20d-470f-b260-67872371eaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(experiment_name=\"compare\")\n",
    "\n",
    "run_ids = []\n",
    "artifact_paths = []\n",
    "model_names = [\n",
    "    \"rugpt3small\", \n",
    "    \"rugpt3medium\", \n",
    "    \"rugpt3large\"\n",
    "]\n",
    "\n",
    "for model, name in zip([rugpt3small, rugpt3medium, rugpt3large], model_names):\n",
    "    with mlflow.start_run(run_name=f\"log_model_{name}\"):\n",
    "        pyfunc_model = model\n",
    "        artifact_path = f\"models/{name}\"\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=artifact_path,\n",
    "            python_model=pyfunc_model,\n",
    "        )\n",
    "        run_ids.append(mlflow.active_run().info.run_id)\n",
    "        artifact_paths.append(artifact_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295619b8-b0c4-43e8-b444-270bb72bf0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eval_df = pd.DataFrame(\n",
    "    {\n",
    "        \"question\": [\n",
    "            \"Как приготовить борщ?\",\n",
    "            \"Столица россии это \",\n",
    "            \"Кто написал Мастер и Маргарита?\",\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "print(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cedcf74-b866-468f-a5a7-02cdecf03922",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(run_ids)):\n",
    "    with mlflow.start_run(\n",
    "        run_id=run_ids[i]\n",
    "    ):  # reopen the run with the stored run ID\n",
    "        evaluation_results = mlflow.evaluate(\n",
    "            model=f\"runs:/{run_ids[i]}/{artifact_paths[i]}\",\n",
    "            model_type=\"text\",\n",
    "            data=eval_df,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8bfe01-7c1b-4cf6-b97c-cbd5ed4fecc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
